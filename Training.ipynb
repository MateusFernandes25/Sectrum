{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4a967a",
   "metadata": {
    "id": "67f78ef8"
   },
   "source": [
    "\n",
    "# Chat Security"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567061e4",
   "metadata": {},
   "source": [
    "This article addresses the development of an information security chatbot through fine-tuning of the Llama (open-source language model). It employs the QLora methodology to adjust the model's weights by retraining them using a database comprised of questions and answers related to information security. The model outperformed the Llama-7B model in Portuguese tasks overall, excelling particularly in Semantic Similarity and Textual Entailment activities. In question-answering tasks, our project approached the average performance of the Sabiá-7B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4668ee9a",
   "metadata": {
    "id": "0a2a3194",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 tensorboard openpyxl scipy boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "340fbf6c",
   "metadata": {
    "id": "2734c034",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_22992\\3765399461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpeft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\peft\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"0.4.0\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m from .auto import (\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mAutoPeftModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\peft\\auto.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m from transformers import (\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[0mAutoModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m from .utils import (\n\u001b[0;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdependency_versions_table\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_version_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mreplace_return_docstrings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[1;33m from .generic import (\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mContextManagers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\generic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_flax_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_torch_fx_proxy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\import_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[0m_torch_fx_available\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m_torch_available\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0mtorch_version\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_torch_version\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     _torch_fx_available = (torch_version.major, torch_version.minor) >= (\n\u001b[0;32m    215\u001b[0m         \u001b[0mTORCH_FX_REQUIRED_VERSION\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\packaging\\version.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(version)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \"\"\"\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mInvalidVersion\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mLegacyVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\packaging\\version.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, version)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[1;31m# Validate the version and parse it into pieces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 264\u001b[1;33m         \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mInvalidVersion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Invalid version: '{version}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import peft\n",
    "import trl\n",
    "import datasets\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoModelForCausalLM,\n",
    "                          AutoTokenizer,\n",
    "                          BitsAndBytesConfig,\n",
    "                          HfArgumentParser,\n",
    "                          TrainingArguments,\n",
    "                          pipeline,\n",
    "                          logging,\n",
    "                          TextStreamer)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c670263",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d780e02b",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "8d8e6f93-90b2-4118-abbc-927397dafc33",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('Número de GPUs:', torch.cuda.device_count())\n",
    "    print('Modelo GPU:', torch.cuda.get_device_name(0))\n",
    "    print('Total Memória [GB] da GPU:',torch.cuda.get_device_properties(0).total_memory / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d0dd55",
   "metadata": {
    "id": "cfe22b67"
   },
   "source": [
    "## Definindo Parâmetros do Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4b3bbfb",
   "metadata": {
    "id": "b47e4f1b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('base_dados_segurança.xlsx')\n",
    "\n",
    "\n",
    "df['finetuning'] = df.apply(lambda x: '<s>[INST]' + x['PERGUNTAS'] + ' [/INST] ' + x['RESPOTAS'] + '<\\s> \\n', axis=1)\n",
    "df['finetuning'].to_csv('data.txt', sep='\\t', index=False)\n",
    "\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6a73170-d1fc-4811-b016-be8a979efc5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d9e31fbd73468baa6caab37a42173b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files=\"data.txt\",split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd779d5",
   "metadata": {
    "id": "4bad0e81",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Quantização\n",
    "\n",
    "use_4bit = True # Ativa precisão 4-bits ao carregar o modelo base\n",
    "bnb_4bit_compute_dtype = \"float16\" # Define o dtype para os parâmetros do modelo base\n",
    "bnb_4bit_quant_type = \"nf4\" # NF4 = Normal Float 4\n",
    "use_nested_quant = False # Desativa a quantização aninhada para modelos básicos de 4 bits (quantização dupla)\n",
    "\n",
    "# Carrega o tokenizer e modelo com configuração QLoRA\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit = use_4bit,\n",
    "                                bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "                                bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
    "                                bnb_4bit_use_double_quant = use_nested_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991db2f1",
   "metadata": {
    "id": "85ded253",
    "tags": []
   },
   "source": [
    "## Carregando o Modelo Base e o Tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db20dbba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "f8e675a1808444199e6da1bfdff5e39f",
      "31a2999b7eac41d19d0ce96cc6b8135a",
      "813a58744c1849fb8af396be094b74c1",
      "7aefb172d8934b3badc464be1ec21ade",
      "3bba45a602ff4657939254cfc488a893",
      "11160001f5ba475383e6a87f10ac477d",
      "bf7517af228949a0b73afcd5778dbb12",
      "d8b6f50b1b594c4fad4dedef827a6342",
      "76eda41fb07642148e2d85c8e624b9ed",
      "9b836ec633844c56ad11d3725a153b94",
      "86eef3fa5f1641bf8c0e7e01a2e4dd15"
     ]
    },
    "id": "1d25ae56",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "02f8a4b1-89fd-472c-f60c-49df62f731e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d347b7c161848f2bcc735b6442a0413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3994c305448d4ff2b6eb8c09781806a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d304ae0d924f54a8d41ada41db4f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3048f6d206204d808374019d1927a61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378f02c5717c4c0ea9306583263af15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32499eaef9ef4f3d847dd0f909d90488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e08a3be1c7a449b88f2b3c06f099be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/179 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nome do modelo BASE\n",
    "modelo_base = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "modelo = AutoModelForCausalLM.from_pretrained(modelo_base,quantization_config = bnb_config,device_map = {\"\": 0})\n",
    "modelo.config.use_cache = False\n",
    "modelo.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c81daa-d0b3-432e-a4ba-e29df885a8ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebcbf5adb8748b09d7b581dd41aafac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9d122291e74b4598b397d90df0b62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5809183fc363448a875e09f32bd95833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5aeffea71d420d9fb8408c6352e4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec7d67112644744aecf287cd496e0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carrega o tokenizador do LLaMA2\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_base, trust_remote_code = True,skip_special_tokens=True, use_auth_token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84947d43-eee2-4a7a-a8e2-708fbd8e66d0",
   "metadata": {
    "id": "85ded253",
    "tags": []
   },
   "source": [
    "## Configurando dados de Treinamento e Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82685916",
   "metadata": {
    "id": "c9ac9bce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LORA\n",
    "lora_r = 64 # Dimensão da atenção LoRA\n",
    "lora_alpha = 16 # Parâmetro Alpha para LoRA scaling\n",
    "lora_dropout = 0.1 # Probabilidade Dropout para as camadas LoRA\n",
    "peft_config = LoraConfig(lora_alpha = lora_alpha,\n",
    "                         lora_dropout = lora_dropout,\n",
    "                         r = lora_r,\n",
    "                         bias = \"none\",\n",
    "                         task_type = \"CAUSAL_LM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da0b3394",
   "metadata": {
    "id": "2bbc6925",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_treinamento = 20\n",
    "epochs = 10\n",
    "optim = \"paged_adamw_32bit\"\n",
    "max_grad_norm = 0.3 # Maximum gradient normal (gradient clipping)\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.001 # Redução de peso a ser aplicada a todas as camadas, exceto pesos bias/LayerNorm\n",
    "lr_scheduler_type = \"cosine\" # Learning rate schedule\n",
    "warmup_ratio = 0.03 # Proporção de passos para um aquecimento linear (de 0 à taxa de aprendizagem)\n",
    "\n",
    "# Habilita o treinamento fp16/bf16\n",
    "fp16 = False\n",
    "bf16 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35529865",
   "metadata": {
    "id": "14761efc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parâmetros de configuração de treino\n",
    "training_arguments = TrainingArguments(output_dir = \"resultados\",\n",
    "                                       num_train_epochs = epochs,\n",
    "                                       per_device_train_batch_size = batch_treinamento,\n",
    "                                       gradient_accumulation_steps = 1, # Número de etapas de atualização para acumular os gradientes\n",
    "                                       optim = optim,\n",
    "                                       save_steps = 0,\n",
    "                                       logging_steps = 25,  # Registrar cada X etapas de atualizações\n",
    "                                       learning_rate = learning_rate,\n",
    "                                       weight_decay = weight_decay,\n",
    "                                       fp16 = fp16,\n",
    "                                       bf16 = bf16,\n",
    "                                       max_grad_norm = max_grad_norm,\n",
    "                                       max_steps = -1, # Número de etapas de treinamento\n",
    "                                       warmup_ratio = warmup_ratio,\n",
    "                                       group_by_length = True, # Agrupar sequências em lotes com o mesmo comprimento\n",
    "                                       lr_scheduler_type = lr_scheduler_type,\n",
    "                                       report_to = \"tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d92793",
   "metadata": {
    "id": "30a82651",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2d610ac8df42ddac2f7931207e6925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir parâmetros de ajuste fino com SFTTrainer\n",
    "trainer = SFTTrainer(model = modelo,\n",
    "                     train_dataset = dataset,\n",
    "                     peft_config = peft_config,\n",
    "                     dataset_text_field = \"text\",\n",
    "                     max_seq_length = None, #Comprimento máximo de sequência a ser usado\n",
    "                     tokenizer = tokenizer,\n",
    "                     args = training_arguments,\n",
    "                     packing = False) # Reunir vários exemplos curtos na mesma sequência de entrada para aumentar a eficiência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc1455a",
   "metadata": {
    "id": "60786d29",
    "tags": []
   },
   "source": [
    "## Treinamento e Ajuste Fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bbe327",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "442c6333",
    "outputId": "c12c9249-46fb-436a-848a-3dbeb5e0b589",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6395, 'learning_rate': 0.00019998159678028148, 'epoch': 0.15}\n",
      "{'loss': 4.0737, 'learning_rate': 0.0001990342096935135, 'epoch': 0.3}\n",
      "{'loss': 0.7401, 'learning_rate': 0.00019666461831457439, 'epoch': 0.44}\n",
      "{'loss': 0.6302, 'learning_rate': 0.0001929068517233169, 'epoch': 0.59}\n",
      "{'loss': 2.5337, 'learning_rate': 0.00018781487421946734, 'epoch': 0.74}\n",
      "{'loss': 0.9528, 'learning_rate': 0.00018146181035548113, 'epoch': 0.89}\n",
      "{'loss': 0.2963, 'learning_rate': 0.00017393889481403784, 'epoch': 1.04}\n",
      "{'loss': 0.6428, 'learning_rate': 0.000165354162210709, 'epoch': 1.18}\n",
      "{'loss': 0.4539, 'learning_rate': 0.00015583089563719985, 'epoch': 1.33}\n",
      "{'loss': 0.6336, 'learning_rate': 0.00014550585622522693, 'epoch': 1.48}\n",
      "{'loss': 0.7579, 'learning_rate': 0.00013452731915580075, 'epoch': 1.63}\n",
      "{'loss': 1.881, 'learning_rate': 0.0001230529443182689, 'epoch': 1.78}\n",
      "{'loss': 0.7043, 'learning_rate': 0.00011124751219802545, 'epoch': 1.92}\n",
      "{'loss': 0.3803, 'learning_rate': 9.92805575072098e-05, 'epoch': 2.07}\n",
      "{'loss': 0.4185, 'learning_rate': 8.732393454120304e-05, 'epoch': 2.22}\n",
      "{'loss': 0.6292, 'learning_rate': 7.554934922419789e-05, 'epoch': 2.37}\n",
      "{'loss': 0.4471, 'learning_rate': 6.412589328548886e-05, 'epoch': 2.51}\n",
      "{'loss': 0.6502, 'learning_rate': 5.32176159775285e-05, 'epoch': 2.66}\n",
      "{'loss': 0.3188, 'learning_rate': 4.298116820767086e-05, 'epoch': 2.81}\n",
      "{'loss': 0.4278, 'learning_rate': 3.356355291560646e-05, 'epoch': 2.96}\n",
      "{'loss': 0.533, 'learning_rate': 2.5100014002729034e-05, 'epoch': 3.11}\n",
      "{'loss': 0.2171, 'learning_rate': 1.771209412996455e-05, 'epoch': 3.25}\n",
      "{'loss': 0.551, 'learning_rate': 1.15058892755187e-05, 'epoch': 3.4}\n"
     ]
    }
   ],
   "source": [
    "# Treina o modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5e995",
   "metadata": {
    "id": "a848cba1"
   },
   "outputs": [],
   "source": [
    "# Salva o modelo ajustado\n",
    "trainer.save_model(\"model/sectrum_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd446f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.config.to_json_file(\"config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84b148c",
   "metadata": {
    "id": "e7486bc8"
   },
   "source": [
    "## Gerando Texto com o Modelo Ajustado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f035c",
   "metadata": {
    "id": "1fc1624b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prepara o prompt\n",
    "prompt = \"Como proteger meu endpoint?\"\n",
    "\n",
    "# Cria o pipeline\n",
    "pipe = pipeline(task = \"text-generation\",\n",
    "                model = modelo,\n",
    "                tokenizer = tokenizer,\n",
    "                max_length = 300,\n",
    "                streamer=TextStreamer(tokenizer,skip_prompt=True))\n",
    "\n",
    "\n",
    "# Executa o pipeline e gera o texto a partir do prompt inicial\n",
    "pipe(f\"[INST] {prompt} [/INST]\")[0]['generated_text']\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
